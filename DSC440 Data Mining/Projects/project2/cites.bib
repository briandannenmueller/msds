
@article{wang_semantic_2016,
	title = {Semantic expansion using word embedding clustering and convolutional neural network for improving short text classification},
	volume = {174},
	issn = {09252312},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231215014502},
	doi = {10.1016/j.neucom.2015.09.096},
	abstract = {Text classiﬁcation can help users to effectively handle and exploit useful information hidden in largescale documents. However, the sparsity of data and the semantic sensitivity to context often hinder the classiﬁcation performance of short texts. In order to overcome the weakness, we propose a uniﬁed framework to expand short texts based on word embedding clustering and convolutional neural network (CNN). Empirically, the semantically related words are usually close to each other in embedding spaces. Thus, we ﬁrst discover semantic cliques via fast clustering. Then, by using additive composition over word embeddings from context with variable window width, the representations of multi-scale semantic units1 in short texts are computed. In embedding spaces, the restricted nearest word embeddings (NWEs)2 of the semantic units are chosen to constitute expanded matrices, where the semantic cliques are used as supervision information. Finally, for a short text, the projected matrix3 and expanded matrices are combined and fed into CNN in parallel. Experimental results on two open benchmarks validate the effectiveness of the proposed method.},
	language = {en},
	urldate = {2019-12-14},
	journal = {Neurocomputing},
	author = {Wang, Peng and Xu, Bo and Xu, Jiaming and Tian, Guanhua and Liu, Cheng-Lin and Hao, Hongwei},
	month = jan,
	year = {2016},
	pages = {806--814},
	file = {Wang 等。 - 2016 - Semantic expansion using word embedding clustering.pdf:files/55/Wang 等。 - 2016 - Semantic expansion using word embedding clustering.pdf:application/pdf}
}

@article{grau_dialogue_nodate,
	title = {Dialogue act classiﬁcation using a {Bayesian} approach},
	abstract = {In this work, we make a contribution to natural speech dialogue act detection. We focus our attention on the dialogue act classiﬁcation using a Bayesian approach. Our classiﬁer is tested on two corpora, the Switchboard and the Basurde tasks. A combination of a naive Bayes classiﬁer and n-grams is used. The impact of different smoothing methods (Laplace and Witten Bell) and n-grams in classiﬁcation are studied.},
	language = {en},
	author = {Grau, Sergio and Sanchis, Emilio and Castro, Marıa Jose and Vilar, David},
	pages = {5},
	file = {Grau 等。 - Dialogue act classiﬁcation using a Bayesian approa.pdf:files/56/Grau 等。 - Dialogue act classiﬁcation using a Bayesian approa.pdf:application/pdf}
}

@article{wilson_recognizing_nodate,
	title = {Recognizing {Contextual} {Polarity} in {Phrase}-{Level} {Sentiment} {Analysis}},
	abstract = {This paper presents a new approach to phrase-level sentiment analysis that ﬁrst determines whether an expression is neutral or polar and then disambiguates the polarity of the polar expressions. With this approach, the system is able to automatically identify the contextual polarity for a large subset of sentiment expressions, achieving results that are signiﬁcantly better than baseline.},
	language = {en},
	author = {Wilson, Theresa and Wiebe, Janyce and Hoffmann, Paul},
	pages = {8},
	file = {Wilson 等。 - Recognizing Contextual Polarity in Phrase-Level Se.pdf:files/57/Wilson 等。 - Recognizing Contextual Polarity in Phrase-Level Se.pdf:application/pdf}
}

@inproceedings{stabek_case_2009,
	address = {Brisbane, Australia},
	title = {The {Case} for a {Consistent} {Cyberscam} {Classification} {Framework} ({CCCF})},
	isbn = {978-1-4244-4902-6},
	url = {http://ieeexplore.ieee.org/document/5319182/},
	doi = {10.1109/UIC-ATC.2009.77},
	abstract = {Cyberscam classification schemes developed by international statistical reporting bodies, including the Bureau of Statistics (Australia), the Internet Crime Complaint Center (US), and the Environics Research Group (Canada), are diverse and largely incompatible. This makes comparisons of cyberscam incidence across jurisdictions very difficult. This paper argues that the critical first step towards the development of an inter-jurisdictional and global approach to identify and intercept cyberscams - and prosecute scammers - is a uniform classification system.},
	language = {en},
	urldate = {2019-12-14},
	booktitle = {2009 {Symposia} and {Workshops} on {Ubiquitous}, {Autonomic} and {Trusted} {Computing}},
	publisher = {IEEE},
	author = {Stabek, Amber and Brown, Simon and Watters, Paul},
	year = {2009},
	pages = {525--530},
	file = {Stabek 等。 - 2009 - The Case for a Consistent Cyberscam Classification.pdf:files/58/Stabek 等。 - 2009 - The Case for a Consistent Cyberscam Classification.pdf:application/pdf}
}

@article{berard_magic_2003,
	title = {The {Magic} {Table}: {Computer}-{Vision} {Based} {Augmentation} of a {Whiteboard} for {Creative} {Meetings}},
	abstract = {This paper presents the “Magic Table”: an augmented whiteboard surface for supporting creative meetings. The Magic Table uses computer vision for scanning and spatially organizing texts and drawings on the surface. Digitization of the physical ink is done by a-posteriori capture of the strokes. The digital information is organized through the manipulation of tokens (small plastic disks). The interaction consist of fast and easy to learn gestures that support multiple simultaneous users and two-handed control.},
	language = {en},
	author = {Bérard, François},
	year = {2003},
	pages = {8},
	file = {Bérard - 2003 - The Magic Table Computer-Vision Based Augmentatio.pdf:files/59/Bérard - 2003 - The Magic Table Computer-Vision Based Augmentatio.pdf:application/pdf}
}

@article{zhang_text_2016,
	title = {Text {Understanding} from {Scratch}},
	url = {http://arxiv.org/abs/1502.01710},
	abstract = {This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classiﬁcation. We constructed several largescale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.},
	language = {en},
	urldate = {2019-12-14},
	journal = {arXiv:1502.01710 [cs]},
	author = {Zhang, Xiang and LeCun, Yann},
	month = apr,
	year = {2016},
	note = {arXiv: 1502.01710},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: This technical report is superseded by a paper entitled "Character-level Convolutional Networks for Text Classification", arXiv:1509.01626. It has considerably more experimental results and a rewritten introduction},
	file = {Zhang 和 LeCun - 2016 - Text Understanding from Scratch.pdf:files/60/Zhang 和 LeCun - 2016 - Text Understanding from Scratch.pdf:application/pdf}
}

@article{topkara_hiding_nodate,
	title = {The {Hiding} {Virtues} of {Ambiguity}: {Quantiﬁably} {Resilient} {Watermarking} of {Natural} {Language} {Text} through {Synonym} {Substitutions}},
	language = {en},
	author = {Topkara, Umut and Topkara, Mercan and Atallah, Mikhail J},
	pages = {11},
	file = {Topkara 等。 - The Hiding Virtues of Ambiguity Quantiﬁably Resil.pdf:files/61/Topkara 等。 - The Hiding Virtues of Ambiguity Quantiﬁably Resil.pdf:application/pdf}
}

@inproceedings{frid-adar_synthetic_2018,
	address = {Washington, DC},
	title = {Synthetic data augmentation using {GAN} for improved liver lesion classification},
	isbn = {978-1-5386-3636-7},
	url = {https://ieeexplore.ieee.org/document/8363576/},
	doi = {10.1109/ISBI.2018.8363576},
	abstract = {In this paper, we present a data augmentation method that generates synthetic medical images using Generative Adversarial Networks (GANs). We propose a training scheme that ﬁrst uses classical data augmentation to enlarge the training set and then further enlarges the data size and its diversity by applying GAN techniques for synthetic data augmentation. Our method is demonstrated on a limited dataset of computed tomography (CT) images of 182 liver lesions (53 cysts, 64 metastases and 65 hemangiomas). The classiﬁcation performance using only classic data augmentation yielded 78.6\% sensitivity and 88.4\% speciﬁcity. By adding the synthetic data augmentation the results signiﬁcantly increased to 85.7\% sensitivity and 92.4\% speciﬁcity.},
	language = {en},
	urldate = {2019-12-14},
	booktitle = {2018 {IEEE} 15th {International} {Symposium} on {Biomedical} {Imaging} ({ISBI} 2018)},
	publisher = {IEEE},
	author = {Frid-Adar, Maayan and Klang, Eyal and Amitai, Michal and Goldberger, Jacob and Greenspan, Hayit},
	month = apr,
	year = {2018},
	pages = {289--293},
	file = {Frid-Adar 等。 - 2018 - Synthetic data augmentation using GAN for improved.pdf:files/62/Frid-Adar 等。 - 2018 - Synthetic data augmentation using GAN for improved.pdf:application/pdf}
}

@article{wei_eda:_2019,
	title = {{EDA}: {Easy} {Data} {Augmentation} {Techniques} for {Boosting} {Performance} on {Text} {Classification} {Tasks}},
	shorttitle = {{EDA}},
	url = {http://arxiv.org/abs/1901.11196},
	abstract = {We present EDA: easy data augmentation techniques for boosting performance on text classiﬁcation tasks. EDA consists of four simple but powerful operations: synonym replacement, random insertion, random swap, and random deletion. On ﬁve text classiﬁcation tasks, we show that EDA improves performance for both convolutional and recurrent neural networks. EDA demonstrates particularly strong results for smaller datasets; on average, across ﬁve datasets, training with EDA while using only 50\% of the available training set achieved the same accuracy as normal training with all available data. We also performed extensive ablation studies and suggest parameters for practical use.},
	language = {en},
	urldate = {2019-12-14},
	journal = {arXiv:1901.11196 [cs]},
	author = {Wei, Jason and Zou, Kai},
	month = aug,
	year = {2019},
	note = {arXiv: 1901.11196},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: EMNLP-IJCNLP 2019 short paper},
	file = {Wei 和 Zou - 2019 - EDA Easy Data Augmentation Techniques for Boostin.pdf:files/63/Wei 和 Zou - 2019 - EDA Easy Data Augmentation Techniques for Boostin.pdf:application/pdf}
}

@article{van_aken_challenges_2018,
	title = {Challenges for {Toxic} {Comment} {Classification}: {An} {In}-{Depth} {Error} {Analysis}},
	shorttitle = {Challenges for {Toxic} {Comment} {Classification}},
	url = {http://arxiv.org/abs/1809.07572},
	abstract = {Toxic comment classiﬁcation has become an active research ﬁeld with many recently proposed approaches. However, while these approaches address some of the task’s challenges others still remain unsolved and directions for further research are needed. To this end, we compare different deep learning and shallow approaches on a new, large comment dataset and propose an ensemble that outperforms all individual models. Further, we validate our ﬁndings on a second dataset. The results of the ensemble enable us to perform an extensive error analysis, which reveals open challenges for state-of-the-art methods and directions towards pending future research. These challenges include missing paradigmatic context and inconsistent dataset labels.},
	language = {en},
	urldate = {2019-12-14},
	journal = {arXiv:1809.07572 [cs]},
	author = {van Aken, Betty and Risch, Julian and Krestel, Ralf and Löser, Alexander},
	month = sep,
	year = {2018},
	note = {arXiv: 1809.07572},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: ALW2: 2nd Workshop on Abusive Language Online to be held at EMNLP 2018 (Brussels, Belgium), October 31st, 2018},
	file = {van Aken 等。 - 2018 - Challenges for Toxic Comment Classification An In.pdf:files/64/van Aken 等。 - 2018 - Challenges for Toxic Comment Classification An In.pdf:application/pdf}
}

@article{taylor_improving_2017,
	title = {Improving {Deep} {Learning} using {Generic} {Data} {Augmentation}},
	url = {http://arxiv.org/abs/1708.06020},
	abstract = {Deep artiﬁcial neural networks require a large corpus of training data in order to effectively learn, where collection of such training data is often expensive and laborious. Data augmentation overcomes this issue by artiﬁcially inﬂating the training set with label preserving transformations. Recently there has been extensive use of generic data augmentation to improve Convolutional Neural Network (CNN) task performance. This study benchmarks various popular data augmentation schemes to allow researchers to make informed decisions as to which training methods are most appropriate for their data sets. Various geometric and photometric schemes are evaluated on a coarse-grained data set using a relatively simple CNN. Experimental results, run using 4-fold cross-validation and reported in terms of Top-1 and Top-5 accuracy, indicate that cropping in geometric augmentation signiﬁcantly increases CNN task performance.},
	language = {en},
	urldate = {2019-12-14},
	journal = {arXiv:1708.06020 [cs, stat]},
	author = {Taylor, Luke and Nitschke, Geoff},
	month = aug,
	year = {2017},
	note = {arXiv: 1708.06020},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Taylor 和 Nitschke - 2017 - Improving Deep Learning using Generic Data Augment.pdf:files/65/Taylor 和 Nitschke - 2017 - Improving Deep Learning using Generic Data Augment.pdf:application/pdf}
}

@article{edunov_understanding_2018,
	title = {Understanding {Back}-{Translation} at {Scale}},
	url = {http://arxiv.org/abs/1808.09381},
	abstract = {An effective method to improve neural machine translation with monolingual data is to augment the parallel training corpus with back-translations of target language sentences. This work broadens the understanding of back-translation and investigates a number of methods to generate synthetic source sentences. We ﬁnd that in all but resource poor settings back-translations obtained via sampling or noised beam outputs are most effective. Our analysis shows that sampling or noisy synthetic data gives a much stronger training signal than data generated by beam or greedy search. We also compare how synthetic data compares to genuine bitext and study various domain effects. Finally, we scale to hundreds of millions of monolingual sentences and achieve a new state of the art of 35 BLEU on the WMT’14 English-German test set.},
	language = {en},
	urldate = {2019-12-14},
	journal = {arXiv:1808.09381 [cs]},
	author = {Edunov, Sergey and Ott, Myle and Auli, Michael and Grangier, David},
	month = oct,
	year = {2018},
	note = {arXiv: 1808.09381},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 12 pages; EMNLP 2018},
	file = {Edunov 等。 - 2018 - Understanding Back-Translation at Scale.pdf:files/66/Edunov 等。 - 2018 - Understanding Back-Translation at Scale.pdf:application/pdf}
}

@article{antoniou_data_2018,
	title = {Data {Augmentation} {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1711.04340},
	abstract = {Effective training of neural networks requires much data. In the low-data regime, parameters are underdetermined, and learnt networks generalise poorly. Data Augmentation (Krizhevsky et al., 2012) alleviates this by using existing data more effectively. However standard data augmentation produces only limited plausible alternative data. Given there is potential to generate a much broader set of augmentations, we design and train a generative model to do data augmentation. The model, based on image conditional Generative Adversarial Networks, takes data from a source domain and learns to take any data item and generalise it to generate other within-class data items. As this generative process does not depend on the classes themselves, it can be applied to novel unseen classes of data. We show that a Data Augmentation Generative Adversarial Network (DAGAN) augments standard vanilla classiﬁers well. We also show a DAGAN can enhance few-shot learning systems such as Matching Networks. We demonstrate these approaches on Omniglot, on EMNIST having learnt the DAGAN on Omniglot, and VGG-Face data. In our experiments we can see over 13\% increase in accuracy in the low-data regime experiments in Omniglot (from 69\% to 82\%), EMNIST (73.9\% to 76\%) and VGG-Face (4.5\% to 12\%); in Matching Networks for Omniglot we observe an increase of 0.5\% (from 96.9\% to 97.4\%) and an increase of 1.8\% in EMNIST (from 59.5\% to 61.3\%).},
	language = {en},
	urldate = {2019-12-14},
	journal = {arXiv:1711.04340 [cs, stat]},
	author = {Antoniou, Antreas and Storkey, Amos and Edwards, Harrison},
	month = mar,
	year = {2018},
	note = {arXiv: 1711.04340},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: 10 pages},
	file = {Antoniou 等。 - 2018 - Data Augmentation Generative Adversarial Networks.pdf:files/67/Antoniou 等。 - 2018 - Data Augmentation Generative Adversarial Networks.pdf:application/pdf}
}

@article{socher_recursive_nodate,
	title = {Recursive {Deep} {Models} for {Semantic} {Compositionality} {Over} a {Sentiment} {Treebank}},
	abstract = {Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes ﬁne grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classiﬁcation from 80\% up to 85.4\%. The accuracy of predicting ﬁne-grained sentiment labels for all phrases reaches 80.7\%, an improvement of 9.7\% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.},
	language = {en},
	author = {Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D and Ng, Andrew and Potts, Christopher},
	pages = {12},
	file = {Socher 等。 - Recursive Deep Models for Semantic Compositionalit.pdf:files/68/Socher 等。 - Recursive Deep Models for Semantic Compositionalit.pdf:application/pdf}
}

@article{hu_mining_nodate,
	title = {Mining {Opinion} {Features} in {Customer} {Reviews}},
	abstract = {It is a common practice that merchants selling products on the Web ask their customers to review the products and associated services. As e-commerce is becoming more and more popular, the number of customer reviews that a product receives grows rapidly. For a popular product, the number of reviews can be in hundreds. This makes it difficult for a potential customer to read them in order to make a decision on whether to buy the product. In this project, we aim to summarize all the customer reviews of a product. This summarization task is different from traditional text summarization because we are only interested in the specific features of the product that customers have opinions on and also whether the opinions are positive or negative. We do not summarize the reviews by selecting or rewriting a subset of the original sentences from the reviews to capture their main points as in the classic text summarization. In this paper, we only focus on mining opinion/product features that the reviewers have commented on. A number of techniques are presented to mine such features. Our experimental results show that these techniques are highly effective.},
	language = {en},
	author = {Hu, Minqing and Liu, Bing},
	pages = {6},
	file = {Hu 和 Liu - Mining Opinion Features in Customer Reviews.pdf:files/69/Hu 和 Liu - Mining Opinion Features in Customer Reviews.pdf:application/pdf}
}

@article{li_learning_2006,
	title = {Learning question classifiers: the role of semantic information},
	volume = {12},
	issn = {1351-3249, 1469-8110},
	shorttitle = {Learning question classifiers},
	url = {https://www.cambridge.org/core/product/identifier/S1351324905003955/type/journal_article},
	doi = {10.1017/S1351324905003955},
	abstract = {To respond correctly to a free form factual question given a large collection of text data, one needs to understand the question to a level that allows determining some of the constraints the question imposes on a possible answer. These constraints may include a semantic classiﬁcation of the sought after answer and may even suggest using diﬀerent strategies when looking for and verifying a candidate answer. This work presents a machine learning approach to question classiﬁcation. Guided by a layered semantic hierarchy of answer types, we develop a hierarchical classiﬁer that classiﬁes questions into ﬁne-grained classes. This work also performs a systematic study of the use of semantic information sources in natural language classiﬁcation tasks. It is shown that, in the context of question classiﬁcation, augmenting the input of the classiﬁer with appropriate semantic category information results in signiﬁcant improvements to classiﬁcation accuracy. We show accurate results on a large collection of free-form questions used in TREC 10 and 11.},
	language = {en},
	number = {3},
	urldate = {2019-12-14},
	journal = {Natural Language Engineering},
	author = {Li, Xin and Roth, Dan},
	month = sep,
	year = {2006},
	pages = {229--249},
	file = {Li 和 Roth - 2006 - Learning question classifiers the role of semanti.pdf:files/70/Li 和 Roth - 2006 - Learning question classifiers the role of semanti.pdf:application/pdf}
}

@inproceedings{pang_sentimental_2004,
	address = {Barcelona, Spain},
	title = {A sentimental education: sentiment analysis using subjectivity summarization based on minimum cuts},
	shorttitle = {A sentimental education},
	url = {http://portal.acm.org/citation.cfm?doid=1218955.1218990},
	doi = {10.3115/1218955.1218990},
	abstract = {Sentiment analysis seeks to identify the viewpoint(s) underlying a text span; an example application is classifying a movie review as “thumbs up” or “thumbs down”. To determine this sentiment polarity, we propose a novel machine-learning method that applies text-categorization techniques to just the subjective portions of the document. Extracting these portions can be implemented using efﬁcient techniques for ﬁnding minimum cuts in graphs; this greatly facilitates incorporation of cross-sentence contextual constraints.},
	language = {en},
	urldate = {2019-12-14},
	booktitle = {Proceedings of the 42nd {Annual} {Meeting} on {Association} for {Computational} {Linguistics}  - {ACL} '04},
	publisher = {Association for Computational Linguistics},
	author = {Pang, Bo and Lee, Lillian},
	year = {2004},
	pages = {271--es},
	file = {Pang 和 Lee - 2004 - A sentimental education sentiment analysis using .pdf:files/71/Pang 和 Lee - 2004 - A sentimental education sentiment analysis using .pdf:application/pdf}
}

@article{jindal_mining_nodate,
	title = {Mining {Comparative} {Sentences} and {Relations}},
	abstract = {This paper studies a text mining problem, comparative sentence mining (CSM). A comparative sentence expresses an ordering relation between two sets of entities with respect to some common features. For example, the comparative sentence “Canon’s optics are better than those of Sony and Nikon” expresses the comparative relation: (better, \{optics\}, \{Canon\}, \{Sony, Nikon\}). Given a set of evaluative texts on the Web, e.g., reviews, forum postings, and news articles, the task of comparative sentence mining is (1) to identify comparative sentences from the texts and (2) to extract comparative relations from the identified comparative sentences. This problem has many applications. For example, a product manufacturer wants to know customer opinions of its products in comparison with those of its competitors. In this paper, we propose two novel techniques based on two new types of sequential rules to perform the tasks. Experimental evaluation has been conducted using different types of evaluative texts from the Web. Results show that our techniques are very promising.},
	language = {en},
	author = {Jindal, Nitin},
	pages = {6},
	file = {Jindal - Mining Comparative Sentences and Relations.pdf:files/72/Jindal - Mining Comparative Sentences and Relations.pdf:application/pdf}
}

@article{xiong_looking_2018,
	title = {Looking {Deeper} into {Deep} {Learning} {Model}: {Attribution}-based {Explanations} of {TextCNN}},
	shorttitle = {Looking {Deeper} into {Deep} {Learning} {Model}},
	url = {http://arxiv.org/abs/1811.03970},
	abstract = {Layer-wise Relevance Propagation (LRP) and saliency maps have been recently used to explain the predictions of Deep Learning models, speciﬁcally in the domain of text classiﬁcation. Given different attribution-based explanations to highlight relevant words for a predicted class label, experiments based on word deleting perturbation is a common evaluation method. This word removal approach, however, disregards any linguistic dependencies that may exist between words or phrases in a sentence, which could semantically guide a classiﬁer to a particular prediction. In this paper, we present a feature-based evaluation framework for comparing the two attribution methods on customer reviews (public data sets) and Customer Due Diligence (CDD) extracted reports (corporate data set). Instead of removing words based on the relevance score, we investigate perturbations based on embedded features removal from intermediate layers of Convolutional Neural Networks. Our experimental study is carried out on embedded-word, embedded-document, and embedded-ngrams explanations. Using the proposed framework, we provide a visualization tool to assist analysts in reasoning toward the model’s ﬁnal prediction.},
	language = {en},
	urldate = {2019-12-14},
	journal = {arXiv:1811.03970 [cs, stat]},
	author = {Xiong, Wenting and Ni'mah, Iftitahu and Huesca, Juan M. G. and van Ipenburg, Werner and Veldsink, Jan and Pechenizkiy, Mykola},
	month = dec,
	year = {2018},
	note = {arXiv: 1811.03970},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Information Retrieval},
	annote = {Comment: NIPS 2018 Workshop on Challenges and Opportunities for AI in Financial Services: the Impact of Fairness, Explainability, Accuracy, and Privacy, Montr{\textbackslash}'eal, Canada},
	file = {Xiong 等。 - 2018 - Looking Deeper into Deep Learning Model Attributi.pdf:files/73/Xiong 等。 - 2018 - Looking Deeper into Deep Learning Model Attributi.pdf:application/pdf}
}

@article{cheng_long_2016,
	title = {Long {Short}-{Term} {Memory}-{Networks} for {Machine} {Reading}},
	url = {http://arxiv.org/abs/1601.06733},
	abstract = {In this paper we address the question of how to render sequence-level networks better at handling structured input. We propose a machine reading simulator which processes text incrementally from left to right and performs shallow reasoning with memory and attention. The reader extends the Long Short-Term Memory architecture with a memory network in place of a single memory cell. This enables adaptive memory usage during recurrence with neural attention, offering a way to weakly induce relations among tokens. The system is initially designed to process a single sequence but we also demonstrate how to integrate it with an encoder-decoder architecture. Experiments on language modeling, sentiment analysis, and natural language inference show that our model matches or outperforms the state of the art.},
	language = {en},
	urldate = {2019-12-14},
	journal = {arXiv:1601.06733 [cs]},
	author = {Cheng, Jianpeng and Dong, Li and Lapata, Mirella},
	month = sep,
	year = {2016},
	note = {arXiv: 1601.06733},
	keywords = {Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: Published as a conference paper at EMNLP 2016},
	file = {Cheng 等。 - 2016 - Long Short-Term Memory-Networks for Machine Readin.pdf:files/74/Cheng 等。 - 2016 - Long Short-Term Memory-Networks for Machine Readin.pdf:application/pdf}
}

@article{devlin_bert:_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be ﬁnetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspeciﬁc architecture modiﬁcations.},
	language = {en},
	urldate = {2019-12-14},
	journal = {arXiv:1810.04805 [cs]},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv: 1810.04805},
	keywords = {Computer Science - Computation and Language},
	file = {Devlin 等。 - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:files/75/Devlin 等。 - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf}
}

@inproceedings{pennington_glove:_2014,
	address = {Doha, Qatar},
	title = {Glove: {Global} {Vectors} for {Word} {Representation}},
	shorttitle = {Glove},
	url = {http://aclweb.org/anthology/D14-1162},
	doi = {10.3115/v1/D14-1162},
	abstract = {Recent methods for learning vector space representations of words have succeeded in capturing ﬁne-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efﬁciently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75\% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.},
	language = {en},
	urldate = {2019-12-14},
	booktitle = {Proceedings of the 2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
	year = {2014},
	pages = {1532--1543},
	file = {Pennington 等。 - 2014 - Glove Global Vectors for Word Representation.pdf:files/76/Pennington 等。 - 2014 - Glove Global Vectors for Word Representation.pdf:application/pdf}
}

@article{rong_word2vec_2016,
	title = {word2vec {Parameter} {Learning} {Explained}},
	url = {http://arxiv.org/abs/1411.2738},
	abstract = {The word2vec model and application by Mikolov et al. have attracted a great amount of attention in recent two years. The vector representations of words learned by word2vec models have been shown to carry semantic meanings and are useful in various NLP tasks. As an increasing number of researchers would like to experiment with word2vec or similar techniques, I notice that there lacks a material that comprehensively explains the parameter learning process of word embedding models in details, thus preventing researchers that are non-experts in neural networks from understanding the working mechanism of such models.},
	language = {en},
	urldate = {2019-12-14},
	journal = {arXiv:1411.2738 [cs]},
	author = {Rong, Xin},
	month = jun,
	year = {2016},
	note = {arXiv: 1411.2738},
	keywords = {Computer Science - Computation and Language},
	file = {Rong - 2016 - word2vec Parameter Learning Explained.pdf:files/77/Rong - 2016 - word2vec Parameter Learning Explained.pdf:application/pdf}
}

@article{liao_textboxes:_2016,
	title = {{TextBoxes}: {A} {Fast} {Text} {Detector} with a {Single} {Deep} {Neural} {Network}},
	shorttitle = {{TextBoxes}},
	url = {http://arxiv.org/abs/1611.06779},
	abstract = {This paper presents an end-to-end trainable fast scene text detector, named TextBoxes, which detects scene text with both high accuracy and efﬁciency in a single network forward pass, involving no post-process except for a standard nonmaximum suppression. TextBoxes outperforms competing methods in terms of text localization accuracy and is much faster, taking only 0.09s per image in a fast implementation. Furthermore, combined with a text recognizer, TextBoxes signiﬁcantly outperforms state-of-the-art approaches on word spotting and end-to-end text recognition tasks.},
	language = {en},
	urldate = {2019-12-14},
	journal = {arXiv:1611.06779 [cs]},
	author = {Liao, Minghui and Shi, Baoguang and Bai, Xiang and Wang, Xinggang and Liu, Wenyu},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.06779},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted by AAAI2017},
	file = {Liao 等。 - 2016 - TextBoxes A Fast Text Detector with a Single Deep.pdf:files/78/Liao 等。 - 2016 - TextBoxes A Fast Text Detector with a Single Deep.pdf:application/pdf}
}

@inproceedings{kim_convolutional_2014,
	address = {Doha, Qatar},
	title = {Convolutional {Neural} {Networks} for {Sentence} {Classification}},
	url = {http://aclweb.org/anthology/D14-1181},
	doi = {10.3115/v1/D14-1181},
	abstract = {We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classiﬁcation tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-speciﬁc vectors through ﬁne-tuning offers further gains in performance. We additionally propose a simple modiﬁcation to the architecture to allow for the use of both task-speciﬁc and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classiﬁcation.},
	language = {en},
	urldate = {2019-12-14},
	booktitle = {Proceedings of the 2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Kim, Yoon},
	year = {2014},
	pages = {1746--1751},
	file = {Kim - 2014 - Convolutional Neural Networks for Sentence Classif.pdf:files/79/Kim - 2014 - Convolutional Neural Networks for Sentence Classif.pdf:application/pdf}
}

@article{abou-assaleh_detection_nodate,
	title = {Detection of {New} {Malicious} {Code} {Using} {N}-grams {Signatures}},
	abstract = {Signature-based malicious code detection is the standard technique in all commercial anti-virus software. This method can detect a virus only after the virus has appeared and caused damage. Signature-based detection performs poorly when attempting to identify new viruses. Motivated by the standard signature-based technique for detecting viruses, and a recent successful text classiﬁcation method, n-grams analysis, we explore the idea of automatically detecting new malicious code. We employ n-grams analysis to automatically generate signatures from malicious and benign software collections. The n-gramsbased signatures are capable of classifying unseen benign and malicious code. The datasets used are large compared to earlier applications of n-grams analysis.},
	language = {en},
	author = {Abou-Assaleh, Tony and Cercone, Nick and Kesˇelj, Vlado and Sweidan, Ray},
	pages = {4},
	file = {Abou-Assaleh 等。 - Detection of New Malicious Code Using N-grams Sign.pdf:files/80/Abou-Assaleh 等。 - Detection of New Malicious Code Using N-grams Sign.pdf:application/pdf}
}

@article{zhou_text_nodate,
	title = {Text {Classification} {Improved} by {Integrating} {Bidirectional} {LSTM} with {Two}-dimensional {Max} {Pooling}},
	abstract = {Recurrent Neural Network (RNN) is one of the most popular architectures used in Natural Language Processsing (NLP) tasks because its recurrent structure is very suitable to process variablelength text. RNN can utilize distributed representations of words by ﬁrst converting the tokens comprising each text into vectors, which form a matrix. And this matrix includes two dimensions: the time-step dimension and the feature vector dimension. Then most existing models usually utilize one-dimensional (1D) max pooling operation or attention-based operation only on the time-step dimension to obtain a ﬁxed-length vector. However, the features on the feature vector dimension are not mutually independent, and simply applying 1D pooling operation over the time-step dimension independently may destroy the structure of the feature representation. On the other hand, applying two-dimensional (2D) pooling operation over the two dimensions may sample more meaningful features for sequence modeling tasks. To integrate the features on both dimensions of the matrix, this paper explores applying 2D max pooling operation to obtain a ﬁxed-length representation of the text. This paper also utilizes 2D convolution to sample more meaningful information of the matrix. Experiments are conducted on six text classiﬁcation tasks, including sentiment analysis, question classiﬁcation, subjectivity classiﬁcation and newsgroup classiﬁcation. Compared with the state-of-the-art models, the proposed models achieve excellent performance on 4 out of 6 tasks. Speciﬁcally, one of the proposed models achieves highest accuracy on Stanford Sentiment Treebank binary classiﬁcation and ﬁne-grained classiﬁcation tasks.},
	language = {en},
	author = {Zhou, Peng and Qi, Zhenyu and Zheng, Suncong and Xu, Jiaming},
	pages = {11},
	file = {Zhou 等。 - Text Classification Improved by Integrating Bidire.pdf:files/81/Zhou 等。 - Text Classification Improved by Integrating Bidire.pdf:application/pdf}
}

@article{wang_elu_2017,
	title = {An {ELU} {Network} with {Total} {Variation} for {Image} {Denoising}},
	url = {http://arxiv.org/abs/1708.04317},
	abstract = {In this paper, we propose a novel convolutional neural network (CNN) for image denoising, which uses exponential linear unit (ELU) as the activation function. We investigate the suitability by analyzing ELU’s connection with trainable nonlinear reaction diffusion model (TNRD) and residual denoising. On the other hand, batch normalization (BN) is indispensable for residual denoising and convergence purpose. However, direct stacking of BN and ELU degrades the performance of CNN. To mitigate this issue, we design an innovative combination of activation layer and normalization layer to exploit and leverage the ELU network, and discuss the corresponding rationale. Moreover, inspired by the fact that minimizing total variation (TV) can be applied to image denoising, we propose a TV regularized L2 loss to evaluate the training effect during the iterations. Finally, we conduct extensive experiments, showing that our model outperforms some recent and popular approaches on Gaussian denoising with speciﬁc or randomized noise levels for both gray and color images.},
	language = {en},
	urldate = {2019-12-14},
	journal = {arXiv:1708.04317 [cs]},
	author = {Wang, Tianyang and Qin, Zhengrui and Zhu, Michelle},
	month = aug,
	year = {2017},
	note = {arXiv: 1708.04317},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 10 pages, Accepted by the 24th International Conference on Neural Information Processing (2017)},
	file = {Wang 等。 - 2017 - An ELU Network with Total Variation for Image Deno.pdf:files/82/Wang 等。 - 2017 - An ELU Network with Total Variation for Image Deno.pdf:application/pdf}
}

@inproceedings{graves_hybrid_2013,
	address = {Olomouc, Czech Republic},
	title = {Hybrid speech recognition with {Deep} {Bidirectional} {LSTM}},
	isbn = {978-1-4799-2756-2},
	url = {http://ieeexplore.ieee.org/document/6707742/},
	doi = {10.1109/ASRU.2013.6707742},
	abstract = {Deep Bidirectional LSTM (DBLSTM) recurrent neural networks have recently been shown to give state-of-the-art performance on the TIMIT speech database. However, the results in that work relied on recurrent-neural-network-speciﬁc objective functions, which are difﬁcult to integrate with existing large vocabulary speech recognition systems. This paper investigates the use of DBLSTM as an acoustic model in a standard neural network-HMM hybrid system. We ﬁnd that a DBLSTM-HMM hybrid gives equally good results on TIMIT as the previous work. It also outperforms both GMM and deep network benchmarks on a subset of the Wall Street Journal corpus. However the improvement in word error rate over the deep network is modest, despite a great increase in framelevel accuracy. We conclude that the hybrid approach with DBLSTM appears to be well suited for tasks where acoustic modelling predominates. Further investigation needs to be conducted to understand how to better leverage the improvements in frame-level accuracy towards better word error rates.},
	language = {en},
	urldate = {2019-12-14},
	booktitle = {2013 {IEEE} {Workshop} on {Automatic} {Speech} {Recognition} and {Understanding}},
	publisher = {IEEE},
	author = {Graves, Alex and Jaitly, Navdeep and Mohamed, Abdel-rahman},
	month = dec,
	year = {2013},
	pages = {273--278},
	file = {Graves 等。 - 2013 - Hybrid speech recognition with Deep Bidirectional .pdf:files/83/Graves 等。 - 2013 - Hybrid speech recognition with Deep Bidirectional .pdf:application/pdf}
}

@article{huang_bidirectional_2015,
	title = {Bidirectional {LSTM}-{CRF} {Models} for {Sequence} {Tagging}},
	url = {http://arxiv.org/abs/1508.01991},
	abstract = {In this paper, we propose a variety of Long Short-Term Memory (LSTM) based models for sequence tagging. These models include LSTM networks, bidirectional LSTM (BI-LSTM) networks, LSTM with a Conditional Random Field (CRF) layer (LSTM-CRF) and bidirectional LSTM with a CRF layer (BI-LSTM-CRF). Our work is the ﬁrst to apply a bidirectional LSTM CRF (denoted as BI-LSTM-CRF) model to NLP benchmark sequence tagging data sets. We show that the BILSTM-CRF model can efﬁciently use both past and future input features thanks to a bidirectional LSTM component. It can also use sentence level tag information thanks to a CRF layer. The BI-LSTMCRF model can produce state of the art (or close to) accuracy on POS, chunking and NER data sets. In addition, it is robust and has less dependence on word embedding as compared to previous observations.},
	language = {en},
	urldate = {2019-12-14},
	journal = {arXiv:1508.01991 [cs]},
	author = {Huang, Zhiheng and Xu, Wei and Yu, Kai},
	month = aug,
	year = {2015},
	note = {arXiv: 1508.01991},
	keywords = {Computer Science - Computation and Language},
	file = {Huang 等。 - 2015 - Bidirectional LSTM-CRF Models for Sequence Tagging.pdf:files/84/Huang 等。 - 2015 - Bidirectional LSTM-CRF Models for Sequence Tagging.pdf:application/pdf}
}

@article{chiu_named_2016,
	title = {Named {Entity} {Recognition} with {Bidirectional} {LSTM}-{CNNs}},
	volume = {4},
	issn = {2307-387X},
	url = {https://www.mitpressjournals.org/doi/abs/10.1162/tacl_a_00104},
	doi = {10.1162/tacl_a_00104},
	abstract = {Named entity recognition is a challenging task that has traditionally required large amounts of knowledge in the form of feature engineering and lexicons to achieve high performance. In this paper, we present a novel neural network architecture that automatically detects word- and character-level features using a hybrid bidirectional LSTM and CNN architecture, eliminating the need for most feature engineering. We also propose a novel method of encoding partial lexicon matches in neural networks and compare it to existing approaches. Extensive evaluation shows that, given only tokenized text and publicly available word embeddings, our system is competitive on the CoNLL-2003 dataset and surpasses the previously reported state of the art performance on the OntoNotes 5.0 dataset by 2.13 F1 points. By using two lexicons constructed from publicly-available sources, we establish new state of the art performance with an F1 score of 91.62 on CoNLL-2003 and 86.28 on OntoNotes, surpassing systems that employ heavy feature engineering, proprietary lexicons, and rich entity linking information.},
	language = {en},
	urldate = {2019-12-14},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Chiu, Jason P.C. and Nichols, Eric},
	month = dec,
	year = {2016},
	pages = {357--370},
	file = {Chiu 和 Nichols - 2016 - Named Entity Recognition with Bidirectional LSTM-C.pdf:files/85/Chiu 和 Nichols - 2016 - Named Entity Recognition with Bidirectional LSTM-C.pdf:application/pdf}
}

@article{li_convergence_nodate,
	title = {Convergence {Analysis} of {Two}-layer {Neural} {Networks} with {ReLU} {Activation}},
	abstract = {In recent years, stochastic gradient descent (SGD) based techniques has become the standard tools for training neural networks. However, formal theoretical understanding of why SGD can train neural networks in practice is largely missing. In this paper, we make progress on understanding this mystery by providing a convergence analysis for SGD on a rich subset of two-layer feedforward networks with ReLU activations. This subset is characterized by a special structure called “identity mapping”.√We prove that, if input follows from Gaussian distribution, with standard O(1/ d) initialization of the weights, SGD converges to the global minimum in polynomial number of steps. Unlike normal vanilla networks, the “identity mapping” makes our network asymmetric and thus the global minimum is unique. To complement our theory, we are also able to show experimentally that multi-layer networks with this mapping have better performance compared with normal vanilla networks.},
	language = {en},
	author = {Li, Yuanzhi and Yuan, Yang},
	pages = {11},
	file = {Li 和 Yuan - Convergence Analysis of Two-layer Neural Networks .pdf:files/86/Li 和 Yuan - Convergence Analysis of Two-layer Neural Networks .pdf:application/pdf}
}

@article{mohammad_is_nodate,
	title = {Is preprocessing of text really worth your time for toxic comment classiﬁcation?},
	abstract = {A large proportion of online comments present on public domains are usually constructive, however a signiﬁcant proportion are toxic in nature. The comments contain lot of typos which increases the number of features manifold, making the ML model difﬁcult to train. Considering the fact that the data scientists spend approximately 80\% of their time in collecting, cleaning and organizing their data [1], we explored how much effort should we invest in the preprocessing (transformation) of raw comments before feeding it to the state-of-the-art classiﬁcation models. With the help of four models on Jigsaw toxic comment classiﬁcation data, we demonstrated that the training of model without any transformation produce relatively decent model. Applying even basic transformations, in some cases, lead to worse performance and should be applied with caution.},
	language = {en},
	author = {Mohammad, Fahim},
	pages = {7},
	file = {Mohammad - Is preprocessing of text really worth your time fo.pdf:files/87/Mohammad - Is preprocessing of text really worth your time fo.pdf:application/pdf}
}

@article{abadi_tensorflow:_2016,
	title = {{TensorFlow}: {Large}-{Scale} {Machine} {Learning} on {Heterogeneous} {Distributed} {Systems}},
	shorttitle = {{TensorFlow}},
	url = {http://arxiv.org/abs/1603.04467},
	abstract = {TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
	language = {en},
	urldate = {2019-12-14},
	journal = {arXiv:1603.04467 [cs]},
	author = {Abadi, Martín and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mane, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viegas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.04467},
	keywords = {Computer Science - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing},
	annote = {Comment: Version 2 updates only the metadata, to correct the formatting of Mart{\textbackslash}'in Abadi's name},
	file = {Abadi 等。 - 2016 - TensorFlow Large-Scale Machine Learning on Hetero.pdf:files/88/Abadi 等。 - 2016 - TensorFlow Large-Scale Machine Learning on Hetero.pdf:application/pdf}
}

@article{schoenauer-sebag_stochastic_2017,
	title = {Stochastic {Gradient} {Descent}: {Going} {As} {Fast} {As} {Possible} {But} {Not} {Faster}},
	shorttitle = {Stochastic {Gradient} {Descent}},
	url = {http://arxiv.org/abs/1709.01427},
	abstract = {When applied to training deep neural networks, stochastic gradient descent (SGD) often incurs steady progression phases, interrupted by catastrophic episodes in which loss and gradient norm explode. A possible mitigation of such events is to slow down the learning process.},
	language = {en},
	urldate = {2019-12-14},
	journal = {arXiv:1709.01427 [cs, stat]},
	author = {Schoenauer-Sebag, Alice and Schoenauer, Marc and Sebag, Michèle},
	month = sep,
	year = {2017},
	note = {arXiv: 1709.01427},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Schoenauer-Sebag 等。 - 2017 - Stochastic Gradient Descent Going As Fast As Poss.pdf:files/89/Schoenauer-Sebag 等。 - 2017 - Stochastic Gradient Descent Going As Fast As Poss.pdf:application/pdf}
}

@article{devlin_bert:_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be ﬁnetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspeciﬁc architecture modiﬁcations.},
	language = {en},
	urldate = {2019-11-20},
	journal = {arXiv:1810.04805 [cs]},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv: 1810.04805},
	keywords = {bert - original, Computer Science - Computation and Language},
	file = {Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:/Users/dotdot/Zotero/storage/KK6SGV4I/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf}
}

@article{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	language = {en},
	urldate = {2019-11-20},
	journal = {arXiv:1706.03762 [cs]},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = dec,
	year = {2017},
	note = {arXiv: 1706.03762},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, transformer - original},
	annote = {Comment: 15 pages, 5 figures},
	file = {Vaswani et al. - 2017 - Attention Is All You Need.pdf:/Users/dotdot/Zotero/storage/57L78H36/Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf}
}

@article{schuster_bidirectional_1997,
	title = {Bidirectional recurrent neural networks},
	volume = {45},
	issn = {1053587X},
	url = {http://ieeexplore.ieee.org/document/650093/},
	doi = {10.1109/78.650093},
	abstract = {In the ﬁrst part of this paper, a regular recurrent neural network (RNN) is extended to a bidirectional recurrent neural network (BRNN). The BRNN can be trained without the limitation of using input information just up to a preset future frame. This is accomplished by training it simultaneously in positive and negative time direction. Structure and training procedure of the proposed network are explained. In regression and classiﬁcation experiments on artiﬁcial data, the proposed structure gives better results than other approaches. For real data, classiﬁcation experiments for phonemes from the TIMIT database show the same tendency.},
	language = {en},
	number = {11},
	urldate = {2019-12-14},
	journal = {IEEE Transactions on Signal Processing},
	author = {Schuster, M. and Paliwal, K.K.},
	month = nov,
	year = {1997},
	pages = {2673--2681},
	file = {Schuster and Paliwal - 1997 - Bidirectional recurrent neural networks.pdf:/Users/dotdot/Zotero/storage/QPEMHHJJ/Schuster and Paliwal - 1997 - Bidirectional recurrent neural networks.pdf:application/pdf}
}