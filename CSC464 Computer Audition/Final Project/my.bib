
@article{low_detection_2011,
	title = {Detection of {Clinical} {Depression} in {Adolescents}’ {Speech} {During} {Family} {Interactions}},
	volume = {58},
	issn = {0018-9294, 1558-2531},
	url = {http://ieeexplore.ieee.org/document/5629355/},
	doi = {10.1109/TBME.2010.2091640},
	abstract = {The properties of acoustic speech have previously been investigated as possible cues for depression in adults. However, these studies were restricted to small populations of patients and the speech recordings were made during patients’ clinical interviews or fixed-text reading sessions. Symptoms of depression often first appear during adolescence at a time when the voice is changing, in both males and females, suggesting that specific studies of these phenomena in adolescent populations are warranted. This study investigated acoustic correlates of depression in a large sample of 139 adolescents (68 clinically depressed and 71 controls). Speech recordings were made during naturalistic interactions between adolescents and their parents. Prosodic, cepstral, spectral, and glottal features, as well as features derived from the Teager energy operator (TEO), were tested within a binary classification framework. Strong gender differences in classification accuracy were observed. The TEO-based features clearly outperformed all other features and feature combinations, providing classification accuracy ranging between 81\%–87\% for males and 72\%–79\% for females. Close, but slightly less accurate, results were obtained by combining glottal features with prosodic and spectral features (67\%–69\% for males and 70\%–75\% for females). These findings indicate the importance of nonlinear mechanisms associated with the glottal flow formation as cues for clinical depression.},
	language = {en},
	number = {3},
	urldate = {2019-12-03},
	journal = {IEEE Transactions on Biomedical Engineering},
	author = {Low, Lu-Shih Alex and Maddage, M C and Lech, M and Sheeber, L B and Allen, N B},
	month = mar,
	year = {2011},
	pages = {574--586},
	file = {Low et al. - 2011 - Detection of Clinical Depression in Adolescents’ S.pdf:/Users/dotdot/Zotero/storage/SZHA7FZW/Low et al. - 2011 - Detection of Clinical Depression in Adolescents’ S.pdf:application/pdf}
}

@article{zhang_speech_2018,
	title = {Speech {Emotion} {Recognition} {Using} {Deep} {Convolutional} {Neural} {Network} and {Discriminant} {Temporal} {Pyramid} {Matching}},
	volume = {20},
	issn = {1520-9210, 1941-0077},
	url = {https://ieeexplore.ieee.org/document/8085174/},
	doi = {10.1109/TMM.2017.2766843},
	abstract = {Speech emotion recognition is challenging because of the affective gap between the subjective emotions and low-level features. Integrating multilevel feature learning and model training, deep convolutional neural networks (DCNN) has exhibited remarkable success in bridging the semantic gap in visual tasks like image classiﬁcation, object detection. This paper explores how to utilize a DCNN to bridge the affective gap in speech signals. To this end, we ﬁrst extract three channels of log Mel-spectrograms (static, delta, and delta delta) similar to the red, green, blue (RGB) image representation as the DCNN input. Then, the AlexNet DCNN model pretrained on the large ImageNet dataset is employed to learn high-level feature representations on each segment divided from an utterance. The learned segmentlevel features are aggregated by a discriminant temporal pyramid matching (DTPM) strategy. DTPM combines temporal pyramid matching and optimal Lp-norm pooling to form a global utterancelevel feature representation, followed by the linear support vector machines for emotion classiﬁcation. Experimental results on four public datasets, that is, EMO-DB, RML, eNTERFACE05, and BAUM-1s, show the promising performance of our DCNN model and the DTPM strategy. Another interesting ﬁnding is that the DCNN model pretrained for image applications performs reasonably good in affective speech feature extraction. Further ﬁne tuning on the target emotional speech datasets substantially promotes recognition performance.},
	language = {en},
	number = {6},
	urldate = {2019-12-03},
	journal = {IEEE Transactions on Multimedia},
	author = {Zhang, Shiqing and Zhang, Shiliang and Huang, Tiejun and Gao, Wen},
	month = jun,
	year = {2018},
	pages = {1576--1590},
	file = {Zhang et al. - 2018 - Speech Emotion Recognition Using Deep Convolutiona.pdf:/Users/dotdot/Zotero/storage/26P8T84Y/Zhang et al. - 2018 - Speech Emotion Recognition Using Deep Convolutiona.pdf:application/pdf}
}

@inproceedings{li_dilated_2019,
	address = {Brighton, United Kingdom},
	title = {Dilated {Residual} {Network} with {Multi}-head {Self}-attention for {Speech} {Emotion} {Recognition}},
	isbn = {978-1-4799-8131-1},
	url = {https://ieeexplore.ieee.org/document/8682154/},
	doi = {10.1109/ICASSP.2019.8682154},
	abstract = {Speech emotion recognition (SER) plays an important role in intelligent speech interaction. One vital challenge in SER is to extract emotion-relevant features from speech signals. In state-of-the-art SER techniques, deep learning methods, e.g, Convolutional Neural Networks (CNNs), are widely employed for feature learning and have achieved signiﬁcant performance. However, in the CNN-oriented methods, two performance limitations have raised: 1) the loss of temporal structure of speech in the progressive resolution reduction; 2) the ignoring of relative dependencies between elements in suprasegmental feature sequence. In this paper, we proposed the combining use of Dilated Residual Network (DRN) and Multi-head Self-attention to alleviate the above limitations. By employing DRN, the network can retain high resolution of temporal structure in feature learning, with similar size of receptive ﬁeld to CNN based approach. By employing Multi-head Self-attention, the network can model the inner dependencies between elements with different positions in the learned suprasegmental feature sequence, which enhances the importing of emotion-salient information. Experiments on emotional benchmarking dataset IEMOCAP have demonstrated the effectiveness of the proposed framework, with 11.7\% to 18.6\% relative improvement to state-of-the-art approaches.},
	language = {en},
	urldate = {2019-12-03},
	booktitle = {{ICASSP} 2019 - 2019 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Li, Runnan and Wu, Zhiyong and Jia, Jia and Zhao, Sheng and Meng, Helen},
	month = may,
	year = {2019},
	pages = {6675--6679},
	file = {Li et al. - 2019 - Dilated Residual Network with Multi-head Self-atte.pdf:/Users/dotdot/Zotero/storage/ZEH6PWMQ/Li et al. - 2019 - Dilated Residual Network with Multi-head Self-atte.pdf:application/pdf}
}

@article{chen_3-d_2018,
	title = {3-{D} {Convolutional} {Recurrent} {Neural} {Networks} {With} {Attention} {Model} for {Speech} {Emotion} {Recognition}},
	volume = {25},
	issn = {1070-9908, 1558-2361},
	url = {https://ieeexplore.ieee.org/document/8421023/},
	doi = {10.1109/LSP.2018.2860246},
	abstract = {Speech emotion recognition (SER) is a difﬁcult task due to the complexity of emotions. The SER performances are heavily dependent on the effectiveness of emotional features extracted from the speech. However, most emotional features are sensitive to emotionally irrelevant factors, such as the speaker, speaking styles, and environment. In this letter, we assume that calculating the deltas and delta-deltas for personalized features not only preserves the effective emotional information but also reduces the inﬂuence of emotionally irrelevant factors, leading to reduce misclassiﬁcation. In addition, SER often suffers from the silent frames and emotionally irrelevant frames. Meanwhile, attention mechanism has exhibited outstanding performances in learning relevant feature representations for speciﬁc tasks. Inspired by this, we propose a three-dimensional attention-based convolutional recurrent neural networks to learn discriminative features for SER, where the Mel-spectrogram with deltas and delta-deltas are used as input. Experiments on IEMOCAP and Emo-DB corpus demonstrate the effectiveness of the proposed method and achieve the state-of-the-art performance in terms of unweighted average recall.},
	language = {en},
	number = {10},
	urldate = {2019-12-03},
	journal = {IEEE Signal Processing Letters},
	author = {Chen, Mingyi and He, Xuanji and Yang, Jing and Zhang, Han},
	month = oct,
	year = {2018},
	pages = {1440--1444},
	file = {Chen et al. - 2018 - 3-D Convolutional Recurrent Neural Networks With A.pdf:/Users/dotdot/Zotero/storage/ICTW3LQT/Chen et al. - 2018 - 3-D Convolutional Recurrent Neural Networks With A.pdf:application/pdf}
}

@article{schuller_relevance_nodate,
	title = {The {Relevance} of {Feature} {Type} for the {Automatic} {Classification} of {Emotional} {User} {States}: {Low} {Level} {Descriptors} and {Functionals}},
	abstract = {In this paper, we report on classiﬁcation results for emotional user states (4 classes, German database of children interacting with a pet robot). Six sites computed acoustic and linguistic features independently from each other, following in part different strategies. A total of 4244 features were pooled together and grouped into 12 low level descriptor types and 6 functional types. For each of these groups, classiﬁcation results using Support Vector Machines and Random Forests are reported for the full set of features, and for 150 features each with the highest individual Information Gain Ratio. The performance for the different groups varies mostly between ≈ 50\% and ≈ 60\%.},
	language = {en},
	author = {Schuller, Bjorn and Batliner, Anton and Seppi, Dino and Steidl, Stefan and Vogt, Thurid and Wagner, Johannes and Devillers, Laurence and Vidrascu, Laurence and Amir, Noam and Kessous, Loic and Aharonson, Vered},
	pages = {4},
	file = {Schuller et al. - The Relevance of Feature Type for the Automatic Cl.pdf:/Users/dotdot/Zotero/storage/MCL4JWSK/Schuller et al. - The Relevance of Feature Type for the Automatic Cl.pdf:application/pdf}
}

@inproceedings{sun_sparse_2019,
	address = {Hsinchu, Taiwan},
	title = {Sparse {Autoencoder} with {Attention} {Mechanism} for {Speech} {Emotion} {Recognition}},
	isbn = {978-1-5386-7884-8},
	url = {https://ieeexplore.ieee.org/document/8771593/},
	doi = {10.1109/AICAS.2019.8771593},
	abstract = {There has been a lot of previous works on speech emotion with machine learning method. However, most of them rely on the effectiveness of labelled speech data. In this paper, we propose a novel algorithm which combines both sparse autoencoder and attention mechanism. The aim is to benefit from labeled and unlabeled data with autoencoder, and to apply attention mechanism to focus on speech frames which have strong emotional information. We can also ignore other speech frames which do not carry emotional content. The proposed algorithm is evaluated on three public databases with crosslanguage system. Experimental results show that the proposed algorithm provide significantly higher accurate predictions compare to existing speech emotion recognition algorithms.},
	language = {en},
	urldate = {2019-12-03},
	booktitle = {2019 {IEEE} {International} {Conference} on {Artificial} {Intelligence} {Circuits} and {Systems} ({AICAS})},
	publisher = {IEEE},
	author = {Sun, Ting-Wei and Wu, An-Yeu Andy},
	month = mar,
	year = {2019},
	pages = {146--149},
	file = {Sun and Wu - 2019 - Sparse Autoencoder with Attention Mechanism for Sp.pdf:/Users/dotdot/Zotero/storage/IIEPA2VR/Sun and Wu - 2019 - Sparse Autoencoder with Attention Mechanism for Sp.pdf:application/pdf}
}

@article{mao_learning_2014,
	title = {Learning {Salient} {Features} for {Speech} {Emotion} {Recognition} {Using} {Convolutional} {Neural} {Networks}},
	volume = {16},
	issn = {1520-9210, 1941-0077},
	url = {http://ieeexplore.ieee.org/document/6913013/},
	doi = {10.1109/TMM.2014.2360798},
	abstract = {As an essential way of human emotional behavior understanding, speech emotion recognition (SER) has attracted a great deal of attention in human-centered signal processing. Accuracy in SER heavily depends on ﬁnding good affect-related, discriminative features. In this paper, we propose to learn affect-salient features for SER using convolutional neural networks (CNN). The training of CNN involves two stages. In the ﬁrst stage, unlabeled samples are used to learn local invariant features (LIF) using a variant of sparse auto-encoder (SAE) with reconstruction penalization. In the second step, LIF is used as the input to a feature extractor, salient discriminative feature analysis (SDFA), to learn affect-salient, discriminative features using a novel objective function that encourages feature saliency, orthogonality, and discrimination for SER. Our experimental results on benchmark datasets show that our approach leads to stable and robust recognition performance in complex scenes (e.g., with speaker and language variation, and environment distortion) and outperforms several well-established SER features.},
	language = {en},
	number = {8},
	urldate = {2019-12-03},
	journal = {IEEE Transactions on Multimedia},
	author = {Mao, Qirong and Dong, Ming and Huang, Zhengwei and Zhan, Yongzhao},
	month = dec,
	year = {2014},
	pages = {2203--2213},
	file = {Mao et al. - 2014 - Learning Salient Features for Speech Emotion Recog.pdf:/Users/dotdot/Zotero/storage/KZYHLTCG/Mao et al. - 2014 - Learning Salient Features for Speech Emotion Recog.pdf:application/pdf}
}

@inproceedings{chang-hyun_park_emotion_2002,
	address = {Beijing, China},
	title = {Emotion recognition of speech based on {RNN}},
	volume = {4},
	isbn = {978-0-7803-7508-6},
	url = {http://ieeexplore.ieee.org/document/1175432/},
	doi = {10.1109/ICMLC.2002.1175432},
	abstract = {Emotion Recognition has various methods. Mainly, it can be performed by visual or aural method. Any boy’s expression face informed others of his emotion. And, when people are talking over the telephone, they can h o w the opposite person’s emotion by only sound data. From this point, we h o w that it is possible to recognize people’s emotion by only sound data. In this paper, we use the pitch of speech as a main feature. And, as the most important thing, we define features of 4SmOtiOnS (normal, angry, laugh, surprise) in pitch analysis. And, based on this feature pattern, we implement a simulator by VC++. First of all, this simulator is composed of ‘Generation of individuals’, ’R”’‘,Evaluation’. And, using the result frnm learning part of this simulator, we can get results applied to other speech data (excepting for learning data). Io detail, Each module uses the following method. First, ‘generation of individnals’-part uses (l+lM))-ES and (l+l)-ES (that is, random). Thus, we observe the comparison result of both methods. Of course, then, we select the best way. Second, ‘RNNO(efurrent Neural Network)’-part is composed of 7nodes. That is, 1-input node, 2-bidden layer nodes, 4-output nodes. Selection of this structure depends on the characteristics of sequentially inputted speech data. Third, ‘evaluation’-part is very important. This part is the cause of the extraction speed and satisfaction degree of result. Then we implement a simulator from above modules. And, applied other speechdata, we observe the result of recognition..},
	language = {en},
	urldate = {2019-12-03},
	booktitle = {Proceedings. {International} {Conference} on {Machine} {Learning} and {Cybernetics}},
	publisher = {IEEE},
	author = {{Chang-Hyun Park} and {Dong-Wook Lee} and {Kwee-Bo Sim}},
	year = {2002},
	pages = {2210--2213},
	file = {Chang-Hyun Park et al. - 2002 - Emotion recognition of speech based on RNN.pdf:/Users/dotdot/Zotero/storage/8IGMJ8KR/Chang-Hyun Park et al. - 2002 - Emotion recognition of speech based on RNN.pdf:application/pdf}
}

@inproceedings{an_emotional_2017,
	address = {Kuala Lumpur},
	title = {Emotional statistical parametric speech synthesis using {LSTM}-{RNNs}},
	isbn = {978-1-5386-1542-3},
	url = {http://ieeexplore.ieee.org/document/8282282/},
	doi = {10.1109/APSIPA.2017.8282282},
	abstract = {This paper studies the methods for emotional statistical parametric speech synthesis (SPSS) using recurrent neural networks (RNN) with long short-term memory (LSTM) units. Two modeling approaches, i.e., emotion-dependent modeling and uniﬁed modeling with emotion codes, are implemented and compared by experiments. In the ﬁrst approach, LSTM-RNNbased acoustic models are built separately for each emotion type. A speaker-independent acoustic model estimated using the speech data from multi-speakers is adopted to initialize the emotion-dependent LSTM-RNNS. Inspired by the speaker code techniques developed for speech recognition and speech synthesis, the second approach builds a uniﬁed LSTM-RNN-based acoustic model using the training data of a variety of emotion types. In the uniﬁed LSTM-RNN model, an emotion code vector is input to all model layers to indicate the emotion characteristics of current utterance. Experimental results on an emotional speech synthesis database with four emotion types (neutral style, happiness, anger, and sadness) show that both approaches achieve signiﬁcant better naturalness of synthetic speech than HMM-based emotiondependent modeling. The emotion-dependent modeling approach outperforms the uniﬁed modeling approach and the HMM-based emotion-dependent modeling in terms of the subjective emotion classiﬁcation rates for synthetic speech. Furthermore, the emotion codes used by the uniﬁed modeling approach are capable of controlling the emotion type and intensity of synthetic speech effectively by interpolating and extrapolating the codes in the training set.},
	language = {en},
	urldate = {2019-12-03},
	booktitle = {2017 {Asia}-{Pacific} {Signal} and {Information} {Processing} {Association} {Annual} {Summit} and {Conference} ({APSIPA} {ASC})},
	publisher = {IEEE},
	author = {An, Shumin and Ling, Zhenhua and Dai, Lirong},
	month = dec,
	year = {2017},
	pages = {1613--1616},
	file = {An et al. - 2017 - Emotional statistical parametric speech synthesis .pdf:/Users/dotdot/Zotero/storage/U44UP2LV/An et al. - 2017 - Emotional statistical parametric speech synthesis .pdf:application/pdf}
}

@inproceedings{wollmer_analyzing_2012,
	address = {Kyoto, Japan},
	title = {Analyzing the memory of {BLSTM} {Neural} {Networks} for enhanced emotion classification in dyadic spoken interactions},
	isbn = {978-1-4673-0046-9 978-1-4673-0045-2 978-1-4673-0044-5},
	url = {http://ieeexplore.ieee.org/document/6288834/},
	doi = {10.1109/ICASSP.2012.6288834},
	abstract = {Recent studies indicate that bidirectional Long Short-Term Memory (BLSTM) recurrent neural networks are well-suited for automatic emotion recognition systems and may lead to better results than systems applying other widely used classiﬁers such as Support Vector Machines or feedforward Neural Networks. The good performance of BLSTM emotion recognition systems could be attributed to their ability to model and exploit contextual information self-learned via recurrently connected memory blocks which allows them to incorporate information about how emotion evolves over time. However, the actual amount of bidirectional context that a BLSTM classiﬁer takes into account when classifying an observation has not been investigated so far. This paper presents a methodology to systematically investigate the number of past and future utterance-level observations that are considered to generate an emotion prediction for a given utterance, and to examine to what extent this temporal bidirectional context contributes to the overall BLSTM performance.},
	language = {en},
	urldate = {2019-12-03},
	booktitle = {2012 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Wollmer, Martin and Metallinou, Angeliki and Katsamanis, Nassos and Schuller, Bjorn and Narayanan, Shrikanth},
	month = mar,
	year = {2012},
	pages = {4157--4160},
	file = {Wollmer et al. - 2012 - Analyzing the memory of BLSTM Neural Networks for .pdf:/Users/dotdot/Zotero/storage/7HXY8VPG/Wollmer et al. - 2012 - Analyzing the memory of BLSTM Neural Networks for .pdf:application/pdf}
}

@inproceedings{zhao_recurrent_2017,
	address = {New Orleans, LA},
	title = {Recurrent convolutional neural network for speech processing},
	isbn = {978-1-5090-4117-6},
	url = {https://ieeexplore.ieee.org/document/7953168/},
	doi = {10.1109/ICASSP.2017.7953168},
	abstract = {Different neural networks have exhibited excellent performance on various speech processing tasks, and they usually have specific advantages and disadvantages. We propose to use a recently developed deep learning model, recurrent convolutional neural network (RCNN), for speech processing, which inherits some merits of recurrent neural network (RNN) and convolutional neural network (CNN). The core module can be viewed as a convolutional layer embedded with an RNN, which enables the model to capture both temporal and frequency dependance in the spectrogram of the speech in an efficient way. The model is tested on speech corpus TIMIT for phoneme recognition and IEMOCAP for emotion recognition. Experimental results show that the model is competitive with previous methods in terms of accuracy and efficiency.},
	language = {en},
	urldate = {2019-12-03},
	booktitle = {2017 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Zhao, Yue and Jin, Xingyu and Hu, Xiaolin},
	month = mar,
	year = {2017},
	pages = {5300--5304},
	file = {Zhao et al. - 2017 - Recurrent convolutional neural network for speech .pdf:/Users/dotdot/Zotero/storage/6464G9FC/Zhao et al. - 2017 - Recurrent convolutional neural network for speech .pdf:application/pdf}
}

@article{zhao_exploring_2019,
	title = {Exploring {Deep} {Spectrum} {Representations} via {Attention}-{Based} {Recurrent} and {Convolutional} {Neural} {Networks} for {Speech} {Emotion} {Recognition}},
	volume = {7},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/8762126/},
	doi = {10.1109/ACCESS.2019.2928625},
	abstract = {The automatic detection of an emotional state from human speech, which plays a crucial role in the area of human–machine interaction, has consistently been shown to be a difﬁcult task for machine learning algorithms. Previous work on emotion recognition has mostly focused on the extraction of carefully hand-crafted and highly engineered features. Results from these works have demonstrated the importance of discriminative spatio-temporal features to model the continual evolutions of different emotions. Recently, spectrogram representations of emotional speech have achieved competitive performance for automatic speech emotion recognition (SER). How machine learning algorithms learn the effective compositional spatio-temporal dynamics for SER has been a fundamental problem of deep representations, herein denoted as deep spectrum representations. In this paper, we develop a model to alleviate this limitation by leveraging a parallel combination of attention-based bidirectional long short-term memory recurrent neural networks with attention-based fully convolutional networks (FCN). The extensive experiments were undertaken on the interactive emotional dyadic motion capture (IEMOCAP) and FAU aibo emotion corpus (FAUAEC) to highlight the effectiveness of our approach. The experimental results indicate that deep spectrum representations extracted from the proposed model are well-suited to the task of SER, achieving a WA of 68.1 \% and a UA of 67.0 \% on IEMOCAP, and 45.4\% for UA on FAU-AEC dataset. Key results indicate that the extracted deep representations combined with a linear support vector classiﬁer are comparable in performance with eGeMAPS and COMPARE, two standard acoustic feature representations.},
	language = {en},
	urldate = {2019-12-03},
	journal = {IEEE Access},
	author = {Zhao, Ziping and Bao, Zhongtian and Zhao, Yiqin and Zhang, Zixing and Cummins, Nicholas and Ren, Zhao and Schuller, Bjorn},
	year = {2019},
	pages = {97515--97525},
	file = {Zhao et al. - 2019 - Exploring Deep Spectrum Representations via Attent.pdf:/Users/dotdot/Zotero/storage/MZAHV7LC/Zhao et al. - 2019 - Exploring Deep Spectrum Representations via Attent.pdf:application/pdf}
}

@article{zhang_attention_2018,
	title = {Attention {Based} {Fully} {Convolutional} {Network} for {Speech} {Emotion} {Recognition}},
	url = {http://arxiv.org/abs/1806.01506},
	doi = {10.23919/APSIPA.2018.8659587},
	abstract = {Speech emotion recognition is a challenging task for three main reasons: 1) human emotion is abstract, which means it is hard to distinguish; 2) in general, human emotion can only be detected in some speciﬁc moments during a long utterance; 3) speech data with emotional labeling is usually limited. In this paper, we present a novel attention based fully convolutional network for speech emotion recognition. We employ fully convolutional network as it is able to handle variable-length speech, free of the demand of segmentation to keep critical information not lost. The proposed attention mechanism can make our model be aware of which time-frequency region of speech spectrogram is more emotion-relevant. Considering limited data, the transfer learning is also adapted to improve the accuracy. Especially, it’s interesting to observe obvious improvement obtained with natural scene image based pre-trained model. Validated on the publicly available IEMOCAP corpus, the proposed model outperformed the state-of-the-art methods with a weighted accuracy of 70.4\% and an unweighted accuracy of 63.9\% respectively.},
	language = {en},
	urldate = {2019-12-03},
	journal = {2018 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)},
	author = {Zhang, Yuanyuan and Du, Jun and Wang, Zirui and Zhang, Jianshu},
	month = nov,
	year = {2018},
	note = {arXiv: 1806.01506},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	pages = {1771--1775},
	file = {Zhang et al. - 2018 - Attention Based Fully Convolutional Network for Sp.pdf:/Users/dotdot/Zotero/storage/KMYSKH7T/Zhang et al. - 2018 - Attention Based Fully Convolutional Network for Sp.pdf:application/pdf}
}

@inproceedings{huang_deep_2017,
	address = {Hong Kong, Hong Kong},
	title = {Deep convolutional recurrent neural network with attention mechanism for robust speech emotion recognition},
	isbn = {978-1-5090-6067-2},
	url = {http://ieeexplore.ieee.org/document/8019296/},
	doi = {10.1109/ICME.2017.8019296},
	abstract = {We present a deep convolutional recurrent neural network for speech emotion recognition based on the log-Mel ﬁlterbank energies, where the convolutional layers are responsible for the discriminative feature learning. Based on the hypothesis that a better understanding of the internal conﬁguration within an utterance would help reduce misclassiﬁcation, we further propose a convolutional attention mechanism to learn the utterance structure relevant to the task. In addition, we quantitatively measure the performance gain contributed by each module in our model in order to characterize the nature of emotion expressed in speech. The experimental results on the eNTERFACE’05 emotion database validate our hypothesis and also demonstrate an absolute improvement by 4.62\% compared to the state-of-the-art approach.},
	language = {en},
	urldate = {2019-12-03},
	booktitle = {2017 {IEEE} {International} {Conference} on {Multimedia} and {Expo} ({ICME})},
	publisher = {IEEE},
	author = {Huang, Che-Wei and Narayanan, Shrikanth Shri},
	month = jul,
	year = {2017},
	pages = {583--588},
	file = {Huang and Narayanan - 2017 - Deep convolutional recurrent neural network with a.pdf:/Users/dotdot/Zotero/storage/77IEGFZQ/Huang and Narayanan - 2017 - Deep convolutional recurrent neural network with a.pdf:application/pdf}
}

@article{xie_speech_2019,
	title = {Speech {Emotion} {Classification} {Using} {Attention}-{Based} {LSTM}},
	volume = {27},
	issn = {2329-9290, 2329-9304},
	url = {https://ieeexplore.ieee.org/document/8752054/},
	doi = {10.1109/TASLP.2019.2925934},
	abstract = {Automatic speech emotion recognition has been a research hotspot in the ﬁeld of human–computer interaction over the past decade. However, due to the lack of research on the inherent temporal relationship of the speech waveform, the current recognition accuracy needs improvement. To make full use of the difference of emotional saturation between time frames, a novel method is proposed for speech recognition using frame-level speech features combined with attention-based long short-term memory (LSTM) recurrent neural networks. Frame-level speech features were extracted from waveform to replace traditional statistical features, which could preserve the timing relations in the original speech through the sequence of frames. To distinguish emotional saturation in different frames, two improvement strategies are proposed for LSTM based on the attention mechanism: ﬁrst, the algorithm reduces the computational complexity by modifying the forgetting gate of traditional LSTM without sacriﬁcing performance and second, in the ﬁnal output of the LSTM, an attention mechanism is applied to both the time and the feature dimension to obtain the information related to the task, rather than using the output from the last iteration of the traditional algorithm. Extensive experiments on the CASIA, eNTERFACE, and GEMEP emotion corpora demonstrate that the performance of the proposed approach is able to outperform the state-of-the-art algorithms reported to date.},
	language = {en},
	number = {11},
	urldate = {2019-12-03},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Xie, Yue and Liang, Ruiyu and Liang, Zhenlin and Huang, Chengwei and Zou, Cairong and Schuller, Bjorn},
	month = nov,
	year = {2019},
	pages = {1675--1685},
	file = {Xie et al. - 2019 - Speech Emotion Classification Using Attention-Base.pdf:/Users/dotdot/Zotero/storage/79JUHP7D/Xie et al. - 2019 - Speech Emotion Classification Using Attention-Base.pdf:application/pdf}
}

@article{meng_speech_2019,
	title = {Speech {Emotion} {Recognition} {From} 3D {Log}-{Mel} {Spectrograms} {With} {Deep} {Learning} {Network}},
	volume = {7},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/8817913/},
	doi = {10.1109/ACCESS.2019.2938007},
	abstract = {Speech emotion recognition is a vital and challenging task that the feature extraction plays a signiﬁcant role in the SER performance. With the development of deep learning, we put our eyes on the structure of end-to-end and authenticate the algorithm that is extraordinary effective. In this paper, we introduce a novel architecture ADRNN (dilated CNN with residual block and BiLSTM based on the attention mechanism) to apply for the speech emotion recognition which can take advantage of the strengths of diverse networks and overcome the shortcomings of utilizing alone, and are evaluated in the popular IEMOCAP database and Berlin EMODB corpus. Dilated CNN can assist the model to acquire more receptive ﬁelds than using the pooling layer. Then, the skip connection can keep more historic info from the shallow layer and BiLSTM layer are adopted to learn long-term dependencies from the learned local features. And we utilize the attention mechanism to enhance further extraction of speech features. Furthermore, we improve the loss function to apply softmax together with the center loss that achieves better classiﬁcation performance. As emotional dialogues are transformed of the spectrograms, we pick up the values of the 3-D Log-Mel spectrums from raw signals and put them into our proposed algorithm and obtain a notable performance to get the 74.96\% unweighted accuracy in the speaker-dependent and the 69.32\% unweighted accuracy in the speaker-independent experiment. It is better than the 64.74\% from previous state-of-the-art methods in the spontaneous emotional speech of the IEMOCAP database. In addition, we propose the networks that achieve recognition accuracies of 90.78\% and 85.39\% on Berlin EMODB of speaker-dependent and speaker-independent experiment respectively, which are better than the accuracy of 88.30\% and 82.82\% obtained by previous work. For validating the robustness and generalization, we also make an experiment for cross-corpus between above databases and get the preferable 63.84\% recognition accuracy in ﬁnal.},
	language = {en},
	urldate = {2019-12-03},
	journal = {IEEE Access},
	author = {Meng, Hao and Yan, Tianhao and Yuan, Fei and Wei, Hongwei},
	year = {2019},
	pages = {125868--125881},
	file = {Meng et al. - 2019 - Speech Emotion Recognition From 3D Log-Mel Spectro.pdf:/Users/dotdot/Zotero/storage/LKDBALRW/Meng et al. - 2019 - Speech Emotion Recognition From 3D Log-Mel Spectro.pdf:application/pdf}
}

@article{vaswani_attention_nodate,
	title = {Attention is {All} you {Need}},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
	language = {en},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	pages = {11},
	file = {Vaswani et al. - Attention is All you Need.pdf:/Users/dotdot/Zotero/storage/6QNBDMEZ/Vaswani et al. - Attention is All you Need.pdf:application/pdf}
}

@inproceedings{poria_convolutional_2016,
	address = {Barcelona, Spain},
	title = {Convolutional {MKL} {Based} {Multimodal} {Emotion} {Recognition} and {Sentiment} {Analysis}},
	isbn = {978-1-5090-5473-2},
	url = {http://ieeexplore.ieee.org/document/7837868/},
	doi = {10.1109/ICDM.2016.0055},
	abstract = {Technology has enabled anyone with an Internet connection to easily create and share their ideas, opinions and content with millions of other people around the world. Much of the content being posted and consumed online is multimodal. With billions of phones, tablets and PCs shipping today with built-in cameras and a host of new video-equipped wearables like Google Glass on the horizon, the amount of video on the Internet will only continue to increase. It has become increasingly difﬁcult for researchers to keep up with this deluge of multimodal content, let alone organize or make sense of it. Mining useful knowledge from video is a critical need that will grow exponentially, in pace with the global growth of content. This is particularly important in sentiment analysis, as both service and product reviews are gradually shifting from unimodal to multimodal. We present a novel method to extract features from visual and textual modalities using deep convolutional neural networks. By feeding such features to a multiple kernel learning classiﬁer, we signiﬁcantly outperform the state of the art of multimodal emotion recognition and sentiment analysis on different datasets.},
	language = {en},
	urldate = {2019-12-03},
	booktitle = {2016 {IEEE} 16th {International} {Conference} on {Data} {Mining} ({ICDM})},
	publisher = {IEEE},
	author = {Poria, Soujanya and Chaturvedi, Iti and Cambria, Erik and Hussain, Amir},
	month = dec,
	year = {2016},
	pages = {439--448},
	file = {Poria et al. - 2016 - Convolutional MKL Based Multimodal Emotion Recogni.pdf:/Users/dotdot/Zotero/storage/3ZNGYJ4V/Poria et al. - 2016 - Convolutional MKL Based Multimodal Emotion Recogni.pdf:application/pdf}
}


@article{busso_iemocap:_2008,
	title = {{IEMOCAP}: interactive emotional dyadic motion capture database},
	volume = {42},
	issn = {1574-020X, 1574-0218},
	shorttitle = {{IEMOCAP}},
	url = {http://link.springer.com/10.1007/s10579-008-9076-6},
	doi = {10.1007/s10579-008-9076-6},
	abstract = {Since emotions are expressed through a combination of verbal and non-verbal channels, a joint analysis of speech and gestures is required to understand expressive human communication. To facilitate such investigations, this paper describes a new corpus named the “interactive emotional dyadic motion capture database” (IEMOCAP), collected by the Speech Analysis and Interpretation Laboratory (SAIL) at the University of Southern California (USC). This database was recorded from ten actors in dyadic sessions with markers on the face, head, and hands, which provide detailed information about their facial expression and hand movements during scripted and spontaneous spoken communication scenarios. The actors performed selected emotional scripts and also improvised hypothetical scenarios designed to elicit speciﬁc types of emotions (happiness, anger, sadness, frustration and neutral state). The corpus contains approximately twelve hours of data. The detailed motion capture information, the interactive setting to elicit authentic emotions, and the size of the database make this corpus a valuable addition to the existing databases in the community for the study and modeling of multimodal and expressive human communication.},
	language = {en},
	number = {4},
	urldate = {2019-12-04},
	journal = {Language Resources and Evaluation},
	author = {Busso, Carlos and Bulut, Murtaza and Lee, Chi-Chun and Kazemzadeh, Abe and Mower, Emily and Kim, Samuel and Chang, Jeannette N. and Lee, Sungbok and Narayanan, Shrikanth S.},
	month = dec,
	year = {2008},
	pages = {335--359},
	file = {Busso et al. - 2008 - IEMOCAP interactive emotional dyadic motion captu.pdf:/Users/dotdot/Zotero/storage/J6W3ADUH/Busso et al. - 2008 - IEMOCAP interactive emotional dyadic motion captu.pdf:application/pdf}
}